\documentclass[11pt]{article}
\usepackage{amsmath, amsthm, amssymb, fancyhdr, fancybox, graphicx}
\usepackage{psfrag, dcolumn, bm, accents, setspace, textcomp}
\usepackage{url, hyperref, color, wasysym}

\pagestyle{empty}

\oddsidemargin  0.05in
%\evensidemargin  7pt
\marginparsep 0pt
\topmargin=-0.5in
\textwidth=6.42in
\textheight=9in
\parskip = 3pt plus 1pt minus 1pt

\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{identity}{Identity}
\newtheorem{defn}{Definition}
\newtheorem{algorithm}{Algorithm}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\dist}{\operatornamewithlimits{\sim}}
\newcommand{\converges}{\operatornamewithlimits{\longrightarrow}}
\newcommand{\cprob}{\stackrel{p}{\longrightarrow}}
\newcommand{\cdist}{\stackrel{d}{\longrightarrow}}
\newcommand{\cprobanddist}{\stackrel{p,d}{\longrightarrow}}
\newcommand{\cas}{\stackrel{a.s.}{\longrightarrow}}
\newcommand{\crth}{\stackrel{r}{\longrightarrow}}
\newcommand{\cone}{\stackrel{1}{\longrightarrow}}
\newcommand{\cms}{\stackrel{m.s.}{\longrightarrow}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\wtssim}{\stackrel{wts}{\sim}}
\newcommand{\simind}{\stackrel{ind}{\sim}}
\newcommand{\eqas}{\stackrel{a.s.}{=}}
\newcommand{\eqdist}{\stackrel{d}{=}}
\def\Var{\textrm{Var}\,}
\def\E{\mathbb{E}\,}
\def\Wish{\textrm{Wishart}\,}
\def\IWish{\textrm{Inv-Wishart}\,}
\def\Pois{\textrm{Poisson}\,}
\def\Nb{\textrm{Neg-Bin}\,}
\def\Gm{\textrm{Gamma}\,}
\def\Expo{\textrm{Expo}\,}
\def\Beta{\textrm{Beta}\,}
\def\Pareto{\textrm{Pareto}\,}
\def\Unif{\textrm{Uniform}\,}
\def\Ber{\textrm{Bernoulli}\,}
\def\Bin{\textrm{Binomial}\,}
\def\s{\mathbf{s}}
\def\Y{\mathbf{Y}}
\def\F{\mathbf{F}}
\def\Ft{\mathbf{F}^{T}}
\def\G{\mathbf{G}}
\def\V{\mathbf{V}}
\def\W{\mathbf{W}}
\def\xt{\mathbf{x}^{T}}
\def\kron{\otimes\,}
\def\R{R} % May change this
\def\G{G} % May change this
\def\I{\mathbb{I}}
\def\J{\mathbb{J}}
\def\1{\mathbf{1}}
\def\0{\mathbf{0}}
\def\R{\texttt{R}}
\def\lme{\texttt{lme4}}
\def\lmer{\texttt{lmer}}
\def\Xi{X_{i}}
\def\Zi{Z_{i}}
\def\diag{\textrm{diag}}
\def\bdiag{\textrm{block-diag}}
\def\dim{\textrm{dim}}
\def\tr{\textrm{tr}}
\def\nstat{n_{\textrm{stations}}}
\def\nrcm{n_{\textrm{RCM}}}
\def\ngcm{n_{\textrm{GCM}}}
\def\ngrid{n_{\textrm{grid}}}

\def\lNS{$\log(N)-\log(S)$}
\def\apl{\alpha_{PL}}
\def\bpl{\beta_{PL}}
\def\Sstar{S_{*}}
\def\Smin{S_{min}}
\def\Yobs{Y_{obs}}
\def\Ymis{Y_{mis}}
\def\Nobs{N_{obs}}
\def\Nmis{N_{mis}}
\def\Ncom{N_{com}}
\def\Iobs{I_{obs}}
\def\Imis{I_{mis}}
\def\Icom{I_{com}}
\def\Sobs{S_{obs}}
\def\Smis{S_{mis}}
\def\Scom{S_{com}}
\def\Eobs{E_{obs}}
\def\Emis{E_{mis}}
\def\Ecom{E_{com}}
\def\Lobs{L_{obs}}
\def\Lmis{L_{mis}}
\def\Lcom{L_{com}}

\def\CUDA{\texttt{CUDA}\,}
\def\Python{\texttt{Python}\,}
\def\PyCUDA{\texttt{PyCUDA}\,}
\def\numpy{\texttt{numpy}\,}
\def\OpenCL{\texttt{OpenCL}\,}
\def\GPUArray{\texttt{GPUArray}\,}

\begin{document}

\title{Nested Sampling and the Evaluation of the \lq{}Evidence\rq{} for Bayesian Model Selection}
\author{
Paul D. Baines, Nicholas Ulle\\
\emph{University of California, Davis}
}
\date{\today}
\maketitle

\section{Introduction}\label{overview}

\begin{itemize}
 \item Explanation of Nested Sampling
 \item Intuitive explanation of the algorithm
 \item Basics of computing the evidence and Bayesian model selection
\end{itemize}

\section*{Example I}
Let:
\begin{align*}
Y_{i} \sim{} N(\mu,\sigma^{2}) , \qquad i=1,\ldots,n ,
\end{align*}
with prior $p(\mu)\propto{}N(\mu_{0},\tau_{0}^{2})$ and $\sigma^{2}$ known. Letting $C=(2\pi)^{-(n+1)/2}(\tau_{0}^{2})^{-1/2}(\sigma^{2})^{-n/2}$, the evidence, or marginal likelihood, is:
\begin{align*}
 p(y) &= \int p(y_{1},\ldots,y_{n}|\mu)p(\mu)d\mu = \int p(\mu)\prod_{i=1}^{n}p(y_{i}|\mu) d\mu \\
 &= \int C \times \exp\left\{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(y_{i}-\mu)^{2} -\frac{1}{2\tau_{0}^{2}}(\mu-\mu_{0})^{2} \right\} d\mu \\
 &= C \times \int \exp\left\{-\frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)\mu^{2} + \frac{1}{2}\mu\left(\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}\right) - \frac{1}{2}\left(\frac{\mu_{0}^{2}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}\right) \right\} d\mu \\
  &= C \times \exp\left\{ - \frac{1}{2}\left(\frac{\mu_{0}^{2}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}\right)
  + \frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1}\left[\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}\right]^{2}\right\} \\
  & \times  \int \exp\left\{ -\frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)\left(\mu - \left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1}\left[\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}\right]\right)^{2} \right\}  d\mu \\
 &=  C\times\exp\left\{ - \frac{1}{2}\left(\frac{\mu_{0}^{2}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}\right)
  + \frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1}\left[\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}\right]^{2}\right\} \times{} (2\pi)^{1/2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1/2} \\
   &= C\times(2\pi)^{1/2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1/2}\exp\left\{ - \frac{1}{2}\left(\frac{\mu_{0}^{2}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}^{2}}{\sigma^{2}}\right)
  + \frac{1}{2}\left(\frac{n}{\sigma^{2}}+\frac{1}{\tau_{0}^{2}}\right)^{-1}\left[\frac{\mu_{0}}{\tau_{0}^{2}} + \frac{\sum_{i=1}^{n}y_{i}}{\sigma^{2}}\right]^{2}\right\} .
\end{align*}
This will allow us to verify the results obtained using nested sampling. In the simple case where $\mu_{0}=0$, $\tau_{0}^{2}=1$, $n=1$, $\sigma^{2}=1$ we obtain:
\begin{align*}
Z = (2\pi)^{-1/2}\left(2\right)^{-1/2}\exp\left\{ - \frac{1}{2}y^{2} + \frac{1}{2}\left(2\right)^{-1}y^{2}\right\}  = \frac{1}{2\sqrt{\pi}}\exp\left\{-\frac{y^{2}}{4}\right\} .
\end{align*}

\section{Example II}
Here we take a look at the classic mixture of normals:
\begin{align*}
 Y_{i} = \sum_{j=1}^{K}I_{ij}Z_{ij} , \qquad i=1,\ldots,n , 
\end{align*}
where:
\begin{align*}
 I_{i} = (I_{i1},\ldots,I_{iK}) &\sim \textrm{Multinomial}\left(1,p\right) , \\ 
 Z_{ij} &\iid N\left(\mu_{j},1\right) .
\end{align*}
The parameters in the model are the mixture proportions $p=(p_{1},\ldots,p_{K})$ (with $\sum_{j}p_{j}=1$) and the mixture locations $\mu=(\mu_{1},\ldots,\mu_{k})$. The number of mixture components $K$ will be fixed for a given model, and we will use the evidence to motivate a model selection procedure to select the appropriate $K$. For convenience we choose conditionally conjugate priors for $\mu$ and $p$:
\begin{align*}
 \mu \sim N\left(\mu_{0},\tau_{0}^{2}\right) , 
 \qquad p \sim \textrm{Dirichlet}\left(\alpha\right) ,
\end{align*}
where $\mu_{0}, \tau_{0}^{2}$ and $\alpha$ are fixed hyperparameters chosen by the analyst.

\subsection{Posterior Distributions}

The $Y_i$ are conditionally independent, so the likelihood is
\begin{align*}
    f(y \vert \mu, p)
    &=
    \prod_{i=1}^n f(y_i \vert \mu, p)
    =
    \prod_{i=1}^n \left[
        \sum_{\text{all } I_i} f(y_i \vert \mu, I_i) f(I_i \vert p)
    \right]
    \\ &=
    \prod_{i=1}^n \left[
        \sum_{j=1}^K (2\pi)^{-1/2} 
        \exp \left(
            -\frac{1}{2} (y_i - \mu_j)^2
        \right)
        p_j
    \right].
\end{align*}
The posterior is therefore
\begin{align*}
    f(\mu, p \vert y)
    &=
    \frac{1}{Z}
    f(y \vert \mu, p) f(\mu) f(p)
    \\ &=
    \frac{1}{Z}
    \left\{
        \prod_{i=1}^n \left[
            \sum_{j=1}^K (2\pi)^{-1/2} 
            \exp \left(
                -\frac{1}{2} (y_i - \mu_j)^2
            \right)
            p_j
        \right]
    \right\}
    \\ &\phantom{=\ \,} \times
    \left\{
        (2\pi\tau_0^2)^{-1/2}
        \exp\left(
            -\frac{1}{2} (\mu - \mu_0 \bm{1})'(\mu - \mu_0 \bm{1})
        \right)
    \right\}
    \\ &\phantom{=\ \,} \times
    \left\{
        \frac{\Gamma\left( \sum_{u=1}^K \alpha_u \right)} 
            {\prod_{v=1}^K \Gamma(\alpha_v)}
        \prod_{j=1}^K p_j^{\alpha_j - 1}
    \right\},
\end{align*}
where $\bm{1}$ denotes the $K \times 1$ vector of ones.

\noindent TODO

\subsection{Evaluating the Evidence: Analytically}

TODO

\subsection{Evaluating the Evidence: Nested Sampling}

TODO

\subsection{Evaluating the Evidence: Other Methods}

TODO



\end{document}
